---
title: "Homework 5"
author: "Matthew Parker"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document

---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

Read in the data and add missing
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species)) %>% 
  janitor::clean_names()
```


Create a function that takes in a vector and does the following: 

* For numeric variables, it fills in missing values with the mean of non-missing values
* For character variables, it fills in missing values with `"virginica"`
```{r}
replace_miss = function(vec) {
  
  if (is.numeric(vec)) {
    if (length(vec) == 1) {
      stop("Missing cannot be replaced for numerical vector with length 1")
    } else {
    out = replace(vec, is.na(vec), round(mean(vec, na.rm = TRUE), 1))
    }
  } else if (is.character(vec)) {
    out = replace(vec, is.na(vec), "virginica")
  } else{
    stop("Missing cannot be replaced unless numerical or character vector")
  }
  
  out
  
}
```

Now apply map statement to apply `replace_miss` function to `iris_with_missing` df to replace missing values.
```{r}
output = map_dfr(iris_with_missing, replace_miss)
```


## Problem 2

Create a dataframe with all the file names, read in all the data, then tidy
```{r, message=FALSE}
file_names_df =
  list.files("./data/", full.names = TRUE) %>% 
  as_tibble() %>% 
  rename("filename" = "value") %>% 
  mutate(
    data = map(.x = filename, read_csv),
    filename = str_remove(filename, "./data//")
  ) %>% 
  separate(filename, into = c("arm", "id"), sep = "_") %>% 
  mutate(
    id = str_remove(id, ".csv"),
    id = as.numeric(id),
    id = if_else(arm == "con", id, id + 10),
    arm = recode(arm, con = "Control", exp = "Experimental")
  ) %>% 
  unnest(data) %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    values_to = "value",
    names_prefix = "week_"
  ) 
```

Spaghetti plot of each subject over time
```{r}
file_names_df %>% 
  ggplot(aes(x = week, y = value, group = id, color = arm)) +
  geom_line() +
  labs(
    title = "Weekly observations for each subject by study arm",
    x = "Week",
    y = "Value"
  )
```

Based on the above plot, it looks like individuals in the experimental arm had their value increase over time, while those in the control arm had a slight decrease in their recorded value.


## Problem 3

Create function to simulate regression and store beta1 estimate and p-value
```{r}
set.seed(1)

sim_regression = function(beta1 = 0) {
  
  sim_data = tibble(
    x = rnorm(30),
    y = 2 + beta1 * x + rnorm(30, 0, 50)
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  ls_fit %>% 
    broom::tidy() %>% 
    filter(term == "x") %>% 
    select(estimate, p.value)
}
```


Simulate regression 10,000 times for beta1 = 0, 1, 2, 3, 4, 5, 6
```{r, cache = TRUE}
sim_results = 
  tibble(beta1 = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = beta1, ~rerun(10000, sim_regression(beta1 = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```


Plot the proportion of times the null was rejected (the power of the test) on the y axis and the true value of Î²2 on the x axis
```{r}
sim_results %>% 
  filter(p.value <= 0.05) %>% 
  group_by(beta1) %>% 
  summarize(
    rej_prop = n() / 10000
  ) %>% 
  ggplot(aes(x = beta1, y = rej_prop)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm") +
  labs(
    y = "Rejection proportion",
    title = "Null hypothesis rejection proportion by beta1"
  )
```

From this plot, as effect size increases, the power increases as well.

Plots comparing average estimate of beta1 hat with true value of beta1
```{r}
all_est_plot =
  sim_results %>% 
  group_by(beta1) %>% 
  summarize(
    avg_est_beta1 = mean(estimate)
  ) %>% 
  ggplot(aes(x = beta1, y = avg_est_beta1)) +
  geom_point() +
  labs(
    title = "Average estimate of beta1 hat by true beta1",
    caption = "All samples"
  )

rej_est_plot = 
  sim_results %>% 
  filter(p.value <= 0.05) %>% 
  group_by(beta1) %>% 
  summarize(
    avg_est_beta1 = mean(estimate)
  ) %>% 
  ggplot(aes(x = beta1, y = avg_est_beta1)) +
  geom_point(color = "red") +
  labs(
    title = "Average estimate of beta1 hat by true beta1",
    caption = "Only samples for which the null was rejected"
  )

all_est_plot + rej_est_plot
```

In the plot comparing the average estimate of beta1 hat with all samples, the estimates appear to be very close to the true value of beta1 hat. However, in the plot comparing the average estimate of beta1 hat with only samples in which the null hypothesis was rejected, the estimates appear to not be close to the true value of beta1, except for when beta1 = 0. This is because in order to reject the null hypothesis of beta1 = 0, the estimated beta1 hat must be sufficiently far from 0. When the true beta1 = 0, we are likely to get an equal number of extreme estimates that are positive and negative. As the true beta1 increases, we will start to get more extreme estimates that are positive, which drive up the estimate of beta1 hat.